{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb19ce-a28d-44cc-b982-417d3d0366c5",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "os.chdir('../')\n",
    "import torch\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from generate_model_seq import SeqGen\n",
    "import torch.optim as optim\n",
    "from general_dataset import GeneralDataset\n",
    "from agi_utils import *\n",
    "from combine_model_seq import SeqCombine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a1e17d-2c29-4671-838a-d2f5185b1e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "assign openagi data path \n",
    "\"\"\"\n",
    "data_path = \"YOUR_DATA_PATH\"\n",
    "\n",
    "task_discriptions = txt_loader(\"./task_description.txt\")\n",
    "# task_idx = [0,21,61,105,110,120,10,35,62,107,115]\n",
    "test_task_idx = [2,3,10,15,20,35,45,55,65,70,70,90,106,107]\n",
    "test_dataloaders = []\n",
    "for i in test_task_idx:\n",
    "    dataset = GeneralDataset(i, data_path)\n",
    "    dataloader = DataLoader(dataset, batch_size=5)\n",
    "    test_dataloaders.append(dataloader)\n",
    "    \n",
    "test_tasks = [task_discriptions[i].strip() for i in test_task_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc460e2-5ad8-44c9-9983-4fa33174c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from undecorated import undecorated\n",
    "from utils import construct_optimizer\n",
    "from types import MethodType\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "backbone_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")#.to(device)\n",
    "backbone_model.load_state_dict(torch.load(\"./finetune/10_shot_finetuned.pt\", map_location=\"cpu\"))\n",
    "backbone_model = backbone_model.to(device)\n",
    "\n",
    "seqGen = SeqGen(backbone_model, tokenizer, device)\n",
    "\n",
    "generate_with_grad = undecorated(seqGen.model.generate)\n",
    "seqGen.model.generate_with_grad = MethodType(generate_with_grad, seqGen.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c5c70-002a-4231-b54f-62d1c06f6540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.utils import io\n",
    "from agi_utils import *\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "from torchvision import transforms\n",
    "from transformers import AutoModel, AutoFeatureExtractor\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "clip_score = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "\n",
    "# Load a pre-trained Vision Transformer model and its feature extractor\n",
    "vit_ckpt = \"nateraw/vit-base-beans\"\n",
    "vit = AutoModel.from_pretrained(vit_ckpt)\n",
    "vit.eval()\n",
    "vit_extractor = AutoFeatureExtractor.from_pretrained(vit_ckpt)\n",
    "\n",
    "f = transforms.ToPILImage()\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "# device_list = [\"cuda:1\",\"cuda:2\",\"cuda:3\",\"cuda:4\",\"cuda:5\",\"cuda:7\",\"cpu\"]\n",
    "device_list = [\"cuda:2\",\"cpu\"]\n",
    "seqCombination = SeqCombine(device_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c304bf7a-c38d-4c00-bbab-d0c378f0e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "clips = []\n",
    "berts = []\n",
    "similairies = []\n",
    "\n",
    "for i, task_description in enumerate(tqdm(test_tasks)):\n",
    "    task_rewards = []\n",
    "    with torch.no_grad():\n",
    "        generated_module_seq, log_prob = seqGen.generate_sequence([test_tasks[i]],\\\n",
    "                                                                   module_length=10, \\\n",
    "                                                                   beam_size=30, \\\n",
    "                                                                   num_seq=30,\\\n",
    "                                                                   top_k=5,\\\n",
    "                                                                   top_p=0.5,\\\n",
    "                                                                   temperature=0.9,\\\n",
    "                                                                   constraint=[0,100],\\\n",
    "                                                                   num_beam_groups=1)\n",
    "\n",
    "    action = torch.argmax(torch.stack(log_prob).detach())\n",
    "    \n",
    "\n",
    "    module_list = generated_module_seq[action][:-1]\n",
    "    # print(task_description)\n",
    "    # print(\"Module Sequence: \" + module_list)\n",
    "\n",
    "    if module_seq_filter(module_list, test_task_idx[i]):\n",
    "        seqCombination.construct_module_seq(module_list)\n",
    "\n",
    "        for idx, batch in enumerate(test_dataloaders[i]):\n",
    "            inputs = list(batch['input'][0])\n",
    "            predictions = seqCombination.run_module_seq(inputs)\n",
    "\n",
    "            if 0<=test_task_idx[i]<=14:\n",
    "                outputs = list(batch['output'][0])\n",
    "                dist = image_similarity(predictions, outputs, vit, vit_extractor)\n",
    "                task_rewards.append(dist/100)\n",
    "            elif 15<=test_task_idx[i]<=104 or 107<=task_idx[i]<=184:\n",
    "                outputs = list(batch['output'][0])\n",
    "                f1 = np.mean(txt_eval(predictions, outputs, bertscore))\n",
    "                \n",
    "                task_rewards.append(f1)\n",
    "            else:\n",
    "                score = clip_score(predictions, inputs)\n",
    "                task_rewards.append(score.detach()/100)\n",
    "                \n",
    "        ave_task_reward = np.mean(task_rewards)    \n",
    "        seqCombination.close_module_seq()\n",
    "            \n",
    "    else:\n",
    "        ave_task_reward = 0\n",
    "        \n",
    "    if 0<=test_task_idx[i]<=14:\n",
    "        similairies.append(ave_task_reward)\n",
    "    elif 15<=test_task_idx[i]<=104 or 107<=test_task_idx[i]:\n",
    "        berts.append(ave_task_reward)\n",
    "    else:\n",
    "        clips.append(ave_task_reward)\n",
    "\n",
    "    rewards.append(ave_task_reward)     \n",
    "    \n",
    "\n",
    "print(\"Finished testing!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9080eca7-0216-41b7-a09f-c3260de693f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(clips), np.mean(berts), np.mean(similairies), np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df8b4ff-32f0-4dae-863d-1f9a009b89ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
